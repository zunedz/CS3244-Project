{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train:  (7767, 563) shape of test:  (3162, 563)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"./input/Train/train.csv\")\n",
    "test = pd.read_csv(\"./input/Test/test.csv\")\n",
    "\n",
    "print(\"shape of train: \", train.shape, \"shape of test: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isnull().values.any())\n",
    "print(test.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: Train data and test data is divided in approximately 70:30. There are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations for Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.Activity.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,10))\n",
    "sns.countplot(x=train.Activity)\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('count')\n",
    "plt.title('Frequency of Activities in Train Set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacked Bar Chart for different activities with regards to subjects\n",
    "\n",
    "stack_group = train.groupby(['subject', 'Activity']).size().unstack()\n",
    "stack_group.plot(kind='bar', stacked=True, figsize=(17, 8), title = 'Activity count w.r.t Subjects in train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations for Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.Activity.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,10))\n",
    "sns.countplot(x=test.Activity)\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('count')\n",
    "plt.title('Frequency of Activities in test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Stacked Bar Chart for different activities with regards to subjects\n",
    "\n",
    "stack_group = test.groupby(['subject', 'Activity']).size().unstack()\n",
    "stack_group.plot(kind='bar', stacked=True, figsize=(17, 8), title = 'Activity count w.r.t Subjects in test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to standardize the data to get better performance\n",
    "features = train.columns.values.tolist()\n",
    "features.remove('Activity')\n",
    "\n",
    "# Separating out the features\n",
    "x = train.loc[:, features].values\n",
    "\n",
    "# Separating out the target\n",
    "y = train.loc[:,['Activity']].values\n",
    "\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "train_features, validation_features, train_labels, validation_labels = train_test_split(x, y, test_size=0.25, random_state=1300)\n",
    "\n",
    "targets = train['Activity'].unique()\n",
    "\n",
    "train_labels = pd.DataFrame(train_labels, columns=['Activity'])\n",
    "validation_labels = pd.DataFrame(validation_labels, columns=['Activity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "We want to use PCA to reduce the the multidimension features in our data into fewer dimensions to better understand the data distribution. We are interested to find out whether the classes are separable using these selected principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Components PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train = pca.fit_transform(train_features)\n",
    "principalDf = pd.DataFrame(data = X_train\n",
    "             , columns = ['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf = pd.concat([principalDf, train_labels], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('PC1', fontsize = 15)\n",
    "ax.set_ylabel('PC2', fontsize = 15)\n",
    "ax.set_title('PCA with 2 components', fontsize = 20)\n",
    "\n",
    "for target in targets:\n",
    "    indicesToKeep = finalDf['Activity'] == target\n",
    "    ax.scatter(finalDf.loc[indicesToKeep, 'PC1']\n",
    "               , finalDf.loc[indicesToKeep, 'PC2']\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result above we can see that the first principal component contains 48.08% of the variance and the second principal component contains 8.11% of the variance. Together, the two components contain 56.19% of the information, which is not very representative. The classes are also overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Components PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca3 = PCA(n_components=3)\n",
    "PC3 = pca3.fit_transform(x)\n",
    "principalDf3 = pd.DataFrame(data = PC3\n",
    "             , columns = ['PC1', 'PC2', 'PC3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDf3 = pd.concat([principalDf3, train[['Activity']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1, projection='3d') \n",
    "ax.set_xlabel('PC1', fontsize = 15)\n",
    "ax.set_ylabel('PC2', fontsize = 15)\n",
    "ax.set_zlabel('PC3', fontsize = 15)\n",
    "ax.set_title('PCA with 3 components', fontsize = 20)\n",
    "targets = train['Activity'].unique()\n",
    "\n",
    "for target in targets:\n",
    "    indicesToKeep = finalDf3['Activity'] == target\n",
    "    ax.scatter(finalDf3.loc[indicesToKeep, 'PC1']\n",
    "               , finalDf3.loc[indicesToKeep, 'PC2']\n",
    "               , finalDf3.loc[indicesToKeep, 'PC3']\n",
    "               , s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca3.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the variance ratio above we can see that the first principal component contains 48.15% of the variance and the second principal component contains 8.08% of the variance, and the third principal component contains 3.19% of the variance. Together, the three components contain 59.42% of the information, which is still considerably small. The classes are again overlapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_n = PCA(n_components=120)\n",
    "PCN = pca_n.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca_n.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isomap Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to standardize the data to get better performance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "features = train.columns.values.tolist()\n",
    "features.remove('Activity')\n",
    "\n",
    "# Separating out the features\n",
    "x = train.loc[:, features].values\n",
    "\n",
    "# Separating out the target\n",
    "y = train.loc[:,['Activity']].values\n",
    "\n",
    "# Standardizing the features\n",
    "x = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import Isomap as ISO\n",
    "\n",
    "iso = ISO(n_components = 2)\n",
    "isomap = iso.fit_transform(x)\n",
    "isomapTempDf = pd.DataFrame(data = isomap, columns = [\"ISO1\", \"ISO2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomapFinalDf = pd.concat([isomapTempDf, train[['Activity']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (16,10))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('ISO1', fontsize = 15)\n",
    "ax.set_ylabel('ISO2', fontsize = 15)\n",
    "ax.set_title('Isomap Embedding', fontsize = 20)\n",
    "targets = train['Activity'].unique()\n",
    "\n",
    "for target in targets:\n",
    "    indicesToKeep = isomapFinalDf['Activity'] == target\n",
    "    ax.scatter(isomapFinalDf.loc[indicesToKeep, 'ISO1']\n",
    "               , isomapFinalDf.loc[indicesToKeep, 'ISO2']\n",
    "               , s = 30\n",
    "               , alpha = 0.5)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbour Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, perplexity = 50, n_iter = 2500)\n",
    "tsne_results = tsne.fit_transform(train_features)\n",
    "\n",
    "df_subset = pd.DataFrame(columns = ['tsne-2d-one','tsne-2d-two', 'Activities'])\n",
    "df_subset['tsne-2d-one'] = tsne_results[:, 0]\n",
    "df_subset['tsne-2d-two'] = tsne_results[:, 1]\n",
    "df_subset['Activities'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 10))\n",
    "plt.title('t-SNE with 2 Components')\n",
    "\n",
    "sns.scatterplot(\n",
    "    x = \"tsne-2d-one\", y = \"tsne-2d-two\",\n",
    "    hue = \"Activities\",\n",
    "    palette = \"bright\",\n",
    "    data = df_subset,\n",
    "    legend = \"full\",\n",
    "    alpha = 0.3\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=120, random_state = 1300)  \n",
    "X_train = pca.fit_transform(train_features)  \n",
    "X_val = pca.transform(validation_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_val, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.score(X_train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on number of trees\n",
    "clf_RF_n = [RandomForestClassifier(n_estimators = 50, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 150, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 200, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 400, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 500, random_state  = 1300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "acc_RF_n = []\n",
    "ex_time_RF_n = []\n",
    "\n",
    "for clf in clf_RF_n:\n",
    "    start_time = timer()\n",
    "    fit = clf.fit(X_train, np.ravel(train_labels))\n",
    "    pred = fit.predict(X_val)\n",
    "    accuracy = accuracy_score(validation_labels, pred)\n",
    "    elapsed = timer() - start_time\n",
    "    \n",
    "    ex_time_RF_n.append(elapsed)\n",
    "    acc_RF_n.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_RF_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical Representation of ACCURACY and EXECUTION Time for RANDOM FOREST CLASSIFIER\n",
    "\n",
    "label = ('n=50','n=150','n=200','n=400','n=500')\n",
    "Accu = acc_RF_n\n",
    "ExTime = ex_time_RF_n\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "y_pos = np.arange(len(label))\n",
    "\n",
    "#Accuracy\n",
    "plt.subplot(2,1,1)\n",
    "plt.bar(y_pos, Accu, align='center')\n",
    "plt.xticks(y_pos, label)\n",
    "plt.ylim(min(Accu)- 0.01 , max(Accu) +0.01)\n",
    "plt.ylabel('Accuracy Percentage')\n",
    "\n",
    "#Execution Time\n",
    "plt.subplot(2,1,2)\n",
    "plt.bar(y_pos, ExTime, align='center', color = 'g')\n",
    "plt.xticks(y_pos, label)\n",
    "plt.ylim(min(ExTime)- 5 , max(ExTime) +5)\n",
    "plt.ylabel('Run-Time(sec)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_RF_d = [RandomForestClassifier(n_estimators = 200, max_depth = 10, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 200, max_depth = 20, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 200, max_depth = 40, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 200, max_depth = 50,random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 200, max_depth = 70, random_state  = 1300),\n",
    "            RandomForestClassifier(n_estimators = 200, max_depth = 200, random_state  = 1300)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_time_RF_d = []\n",
    "acc_RF_d = []\n",
    "\n",
    "for clf in clf_RF_d :\n",
    "    start_time = timer()\n",
    "    fit = clf.fit(X_train, np.ravel(train_labels))\n",
    "    pred = fit.predict(X_test)\n",
    "    accuracy = accuracy_score(validation_labels, pred)\n",
    "    elapsed = timer() - start_time\n",
    "    \n",
    "    ex_time_RF_d.append(elapsed)\n",
    "    acc_RF_d.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphical Representation of ACCURACY and Execution Time for Random Forest with n = 200 and different max depths of a tree.\n",
    "\n",
    "label = ('d =10','d =20','d =40','d =50','d =70', ' d =200')\n",
    "Accu = acc_RF_d\n",
    "ExTime = ex_time_RF_d\n",
    "\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "y_pos = np.arange(len(label))\n",
    "\n",
    "#Accuracy\n",
    "plt.subplot(2,1,1)\n",
    "plt.bar(y_pos, Accu, align='center')\n",
    "plt.xticks(y_pos, label)\n",
    "plt.ylim(min(Accu)- 0.01 , max(Accu) +0.01)\n",
    "plt.ylabel('Accuracy Percentage')\n",
    "\n",
    "#Execution Time\n",
    "plt.subplot(2,1,2)\n",
    "plt.bar(y_pos, ExTime, align='center', color = 'g')\n",
    "plt.xticks(y_pos, label)\n",
    "plt.ylim(min(ExTime)- 5 , max(ExTime) +5)\n",
    "plt.ylabel('Run-Time(sec)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the current model (Random Forest) based on F1 Score, run time Recall and Precision\n",
    "\n",
    "**Will add the rest once andrea adds the smote function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import package used\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the data for training\n",
    "\n",
    "#Separating features and response for training data\n",
    "xtrain_features = train.iloc[:,:-1]\n",
    "xtrain_label = train['Activity']\n",
    "\n",
    "#Separating features and response for test data\n",
    "test_features = train.iloc[:,:-1]\n",
    "test_label = train['Activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of labels\n",
    "activity_label = pd.Series(test.Activity.values).unique()\n",
    "\n",
    "activity_label_fixed = ['STANDING', 'SITTING', 'LAYING', 'WALKING', 'WALKING_DOWNSTAIRS',\n",
    "       'WALKING_UPSTAIRS']\n",
    "\n",
    "activity_label_moving = ['STAND_TO_SIT', 'SIT_TO_STAND',\n",
    "       'STAND_TO_LIE', 'LIE_TO_SIT', 'SIT_TO_LIE',\n",
    "       'LIE_TO_STAND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert everything to numpy array to fit into the model\n",
    "X_labels = np.array(xtrain_label)\n",
    "X_features = np.array(xtrain_features)\n",
    "\n",
    "test_labels = np.array(test_label)\n",
    "test_features = np.array(test_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting train data into validation and train, at using 20-80 ratio, with random state = 1300\n",
    "\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(X_features, X_labels, test_size=0.2, random_state=1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features: (6213, 562) train_labels:  (6213,) val_features:  (1554, 562) val_labels: (1554,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_features:\", train_features.shape, \"train_labels: \", train_labels.shape , \"val_features: \", val_features.shape, \"val_labels:\" , val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do Dimentionality Reduction, Principal Component Analysis, with n_components = 120 (As found above), with random state = 1300\n",
    "# Do it after splitting to prevent snooping\n",
    "\n",
    "pca = PCA(n_components=120, random_state = 1300)  \n",
    "X_train = pca.fit_transform(train_features)  \n",
    "X_val = pca.transform(val_features)  \n",
    "X_test = pca.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RandomForest with depth = 40, random state = 1300 for validation set\n",
    "clf = RandomForestClassifier(n_estimators = 150, max_depth = 40, max_features= 'log2', random_state = 1300)\n",
    "\n",
    "start_time = timer()\n",
    "fit = clf.fit(train_features, train_labels)\n",
    "pred_val = fit.predict(val_features)\n",
    "\n",
    "\n",
    "#Get evaluation metric\n",
    "accuracy_val = accuracy_score(val_labels, pred_val)\n",
    "elapsed_val = timer() - start_time\n",
    "f1_val = f1_score(val_labels, pred_val, average = None, labels = activity_label)\n",
    "avgf1_val = f1_score(val_labels, pred_val, average = \"micro\") #Using micro as labels are under represented\n",
    "recall_val = recall_score(val_labels, pred_val, average = None, labels = activity_label)\n",
    "avgrecall_val = recall_score(val_labels, pred_val, average = \"micro\") #Using micro as labels are under represented\n",
    "precision_val = precision_score(val_labels, pred_val, average = None, labels = activity_label)\n",
    "avgprec_val = precision_score(val_labels, pred_val, average = \"micro\") #Using micro as labels are under represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Validation Set:  0.9716859716859717\n",
      "Average Precision Score of Validation Set:  0.9716859716859717\n",
      "Average Recall Score of Validation Set:  0.9716859716859717\n",
      "Average F1 Score of Validation Set:  0.9716859716859717\n",
      "Run Time of Validation Set: 5.750156599999997\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Validation Set: \", accuracy_val)\n",
    "print(\"Average Precision Score of Validation Set: \", avgprec_val)\n",
    "print(\"Average Recall Score of Validation Set: \", avgrecall_val)\n",
    "print(\"Average F1 Score of Validation Set: \", avgf1_val)\n",
    "print(\"Run Time of Validation Set:\", elapsed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STANDING</th>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.967851</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.994652</td>\n",
       "      <td>0.987835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall Score</th>\n",
       "      <td>0.976109</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision Score</th>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.985437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STANDING  STAND_TO_SIT   SITTING  SIT_TO_STAND  STAND_TO_LIE  \\\n",
       "F1 Score         0.967851      0.444444  0.963504      0.666667      0.756757   \n",
       "Recall Score     0.976109      0.333333  0.956522      0.500000      0.823529   \n",
       "Precision Score  0.959732      0.666667  0.970588      1.000000      0.700000   \n",
       "\n",
       "                 LAYING  LIE_TO_SIT  SIT_TO_LIE  LIE_TO_STAND  WALKING  \\\n",
       "F1 Score            1.0    0.740741    0.709677      0.421053      1.0   \n",
       "Recall Score        1.0    0.909091    0.733333      0.285714      1.0   \n",
       "Precision Score     1.0    0.625000    0.687500      0.800000      1.0   \n",
       "\n",
       "                 WALKING_DOWNSTAIRS  WALKING_UPSTAIRS  \n",
       "F1 Score                   0.994652          0.987835  \n",
       "Recall Score               1.000000          0.990244  \n",
       "Precision Score            0.989362          0.985437  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_val] + [recall_val] + [precision_val], columns = activity_label, index = [\"F1 Score\", \"Recall Score\", \"Precision Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Validation set, it seems like the scores for the average and accuracy seem to be acceptable (Around 97%)\n",
    "\n",
    "But when we look at the individual F1/Precision/Recall score, it seems that the transitional labels all generally have signficantly lower score compared to the stationary action labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RandomForest with depth = 40, random state = 1300 for test set\n",
    "\n",
    "start_time = timer()\n",
    "fit_test = clf.fit(train_features, train_labels)\n",
    "pred_test = fit_test.predict(test_features)\n",
    "\n",
    "#Get etestuation metric\n",
    "accuracy_test = accuracy_score(test_labels, pred_test)\n",
    "elapsed_test = timer() - start_time\n",
    "f1_test = f1_score(test_labels, pred_test, average = None, labels = activity_label)\n",
    "avgf1_test = f1_score(test_labels, pred_test, average = \"macro\") #Using micro as labels are under represented\n",
    "recall_test = recall_score(test_labels, pred_test, average = None, labels = activity_label)\n",
    "avgrecall_test = recall_score(test_labels, pred_test, average = \"micro\") #Using micro as labels are under represented\n",
    "precision_test = precision_score(test_labels, pred_test, average = None, labels = activity_label)\n",
    "avgprec_test = precision_score(test_labels, pred_test, average = \"micro\") #Using micro as labels are under represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Set:  0.9943350070812411\n",
      "Average Precision Score of Test Set:  0.9943350070812411\n",
      "Average Recall Score of Test Set:  0.9943350070812411\n",
      "Average F1 Score of Test Set:  0.969628050235721\n",
      "Run Time of Test Set: 5.660717499999997\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Test Set: \", accuracy_test)\n",
    "print(\"Average Precision Score of Test Set: \", avgprec_test)\n",
    "print(\"Average Recall Score of Test Set: \", avgrecall_test)\n",
    "print(\"Average F1 Score of Test Set: \", avgf1_test)\n",
    "print(\"Run Time of Test Set:\", elapsed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STANDING</th>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.993336</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.992254</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>0.997671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall Score</th>\n",
       "      <td>0.995081</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.990719</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision Score</th>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.993794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.997978</td>\n",
       "      <td>0.997207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STANDING  STAND_TO_SIT   SITTING  SIT_TO_STAND  STAND_TO_LIE  \\\n",
       "F1 Score         0.993336      0.945055  0.992254      0.977778      0.950820   \n",
       "Recall Score     0.995081      0.914894  0.990719      0.956522      0.966667   \n",
       "Precision Score  0.991597      0.977273  0.993794      1.000000      0.935484   \n",
       "\n",
       "                 LAYING  LIE_TO_SIT  SIT_TO_LIE  LIE_TO_STAND  WALKING  \\\n",
       "F1 Score            1.0    0.944000    0.940397      0.895238      1.0   \n",
       "Recall Score        1.0    0.983333    0.946667      0.824561      1.0   \n",
       "Precision Score     1.0    0.907692    0.934211      0.979167      1.0   \n",
       "\n",
       "                 WALKING_DOWNSTAIRS  WALKING_UPSTAIRS  \n",
       "F1 Score                   0.998988          0.997671  \n",
       "Recall Score               1.000000          0.998136  \n",
       "Precision Score            0.997978          0.997207  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_test] + [recall_test] + [precision_test], columns = activity_label, index = [\"F1 Score\", \"Recall Score\", \"Precision Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to check using the PCA data instead to see if there are any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RandomForest with depth = 40, random state = 1300 for PCA validation set\n",
    "clf = RandomForestClassifier(n_estimators = 150, max_depth = 40, max_features= 'log2', random_state = 1300)\n",
    "\n",
    "start_time = timer()\n",
    "fit = clf.fit(X_train, train_labels)\n",
    "pred_val2 = fit.predict(X_val)\n",
    "\n",
    "\n",
    "#Get evaluation metric\n",
    "accuracy_val2 = accuracy_score(val_labels, pred_val2)\n",
    "elapsed_val2 = timer() - start_time\n",
    "f1_val2 = f1_score(val_labels, pred_val2, average = None, labels = activity_label)\n",
    "avgf1_val2 = f1_score(val_labels, pred_val2, average = \"micro\") #Using micro as labels are under represented\n",
    "recall_val2 = recall_score(val_labels, pred_val2, average = None, labels = activity_label)\n",
    "avgrecall_val2 = recall_score(val_labels, pred_val2, average = \"micro\") #Using micro as labels are under represented\n",
    "precision_val2 = precision_score(val_labels, pred_val2, average = None, labels = activity_label, zero_division = 0)\n",
    "avgprec_val2 = precision_score(val_labels, pred_val2, average = \"micro\") #Using micro as labels are under represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, a warning appeared as the label \"SIT_TO_STAND\" was predicted 0 times. Hence TP + FP = 0, causing precision to give an warning. So it was set to 0 by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label that had 0 predictions:  {'SIT_TO_STAND'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Label that had 0 predictions: \", set(val_labels) - set(pred_val2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Validation Set:  0.9247104247104247\n",
      "Average Precision Score of Validation Set:  0.9247104247104247\n",
      "Average Recall Score of Validation Set:  0.9247104247104247\n",
      "Average F1 Score of Validation Set:  0.9247104247104247\n",
      "Run Time of Validation Set: 4.78855139999996\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Validation Set: \", accuracy_val2)\n",
    "print(\"Average Precision Score of Validation Set: \", avgprec_val2)\n",
    "print(\"Average Recall Score of Validation Set: \", avgrecall_val2)\n",
    "print(\"Average F1 Score of Validation Set: \", avgf1_val2)\n",
    "print(\"Run Time of Validation Set:\", elapsed_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STANDING</th>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.900813</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.872865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.985386</td>\n",
       "      <td>0.949333</td>\n",
       "      <td>0.944578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall Score</th>\n",
       "      <td>0.945392</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.956098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision Score</th>\n",
       "      <td>0.860248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.979253</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STANDING  STAND_TO_SIT   SITTING  SIT_TO_STAND  STAND_TO_LIE  \\\n",
       "F1 Score         0.900813      0.500000  0.872865           0.0      0.645161   \n",
       "Recall Score     0.945392      0.333333  0.833333           0.0      0.588235   \n",
       "Precision Score  0.860248      1.000000  0.916335           0.0      0.714286   \n",
       "\n",
       "                   LAYING  LIE_TO_SIT  SIT_TO_LIE  LIE_TO_STAND   WALKING  \\\n",
       "F1 Score         0.989796    0.500000    0.583333      0.400000  0.985386   \n",
       "Recall Score     1.000000    0.545455    0.466667      0.285714  0.991597   \n",
       "Precision Score  0.979798    0.461538    0.777778      0.666667  0.979253   \n",
       "\n",
       "                 WALKING_DOWNSTAIRS  WALKING_UPSTAIRS  \n",
       "F1 Score                   0.949333          0.944578  \n",
       "Recall Score               0.956989          0.956098  \n",
       "Precision Score            0.941799          0.933333  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_val2] + [recall_val2] + [precision_val2], columns = activity_label, index = [\"F1 Score\", \"Recall Score\", \"Precision Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RandomForest with depth = 40, random state = 1300 for test set\n",
    "\n",
    "start_time = timer()\n",
    "fit_test2 = clf.fit(X_train, train_labels)\n",
    "pred_test2 = fit_test2.predict(X_test)\n",
    "\n",
    "#Get etest2uation metric\n",
    "accuracy_test2 = accuracy_score(test_labels, pred_test2)\n",
    "elapsed_test2 = timer() - start_time\n",
    "f1_test2 = f1_score(test_labels, pred_test2, average = None, labels = activity_label)\n",
    "avgf1_test2 = f1_score(test_labels, pred_test2, average = \"macro\") #Using micro as labels are under represented\n",
    "recall_test2 = recall_score(test_labels, pred_test2, average = None, labels = activity_label)\n",
    "avgrecall_test2 = recall_score(test_labels, pred_test2, average = \"micro\") #Using micro as labels are under represented\n",
    "precision_test2 = precision_score(test_labels, pred_test2, average = None, labels = activity_label)\n",
    "avgprec_test2 = precision_score(test_labels, pred_test2, average = \"micro\") #Using micro as labels are under represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Test Set:  0.984936268829664\n",
      "Average Precision Score of Test Set:  0.984936268829664\n",
      "Average Recall Score of Test Set:  0.984936268829664\n",
      "Average F1 Score of Test Set:  0.9578574361023887\n",
      "Run Time of Test Set: 5.188065999999935\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Test Set: \", accuracy_test2)\n",
    "print(\"Average Precision Score of Test Set: \", avgprec_test2)\n",
    "print(\"Average Recall Score of Test Set: \", avgrecall_test2)\n",
    "print(\"Average F1 Score of Test Set: \", avgf1_test2)\n",
    "print(\"Run Time of Test Set:\", elapsed_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STANDING</th>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.978783</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.973838</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.937853</td>\n",
       "      <td>0.997881</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.997149</td>\n",
       "      <td>0.990389</td>\n",
       "      <td>0.989307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall Score</th>\n",
       "      <td>0.988756</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.964424</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.998369</td>\n",
       "      <td>0.991895</td>\n",
       "      <td>0.991612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision Score</th>\n",
       "      <td>0.969008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.995772</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.995932</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STANDING  STAND_TO_SIT   SITTING  SIT_TO_STAND  STAND_TO_LIE  \\\n",
       "F1 Score         0.978783      0.955556  0.973838      0.954545      0.937853   \n",
       "Recall Score     0.988756      0.914894  0.964424      0.913043      0.922222   \n",
       "Precision Score  0.969008      1.000000  0.983438      1.000000      0.954023   \n",
       "\n",
       "                   LAYING  LIE_TO_SIT  SIT_TO_LIE  LIE_TO_STAND   WALKING  \\\n",
       "F1 Score         0.997881    0.901639    0.930556      0.886792  0.997149   \n",
       "Recall Score     1.000000    0.916667    0.893333      0.824561  0.998369   \n",
       "Precision Score  0.995772    0.887097    0.971014      0.959184  0.995932   \n",
       "\n",
       "                 WALKING_DOWNSTAIRS  WALKING_UPSTAIRS  \n",
       "F1 Score                   0.990389          0.989307  \n",
       "Recall Score               0.991895          0.991612  \n",
       "Precision Score            0.988889          0.987013  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_test2] + [recall_test2] + [precision_test2], columns = activity_label, index = [\"F1 Score\", \"Recall Score\", \"Precision Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between the 4 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Avg Precision</th>\n",
       "      <th>Avg Recall</th>\n",
       "      <th>Avg F1</th>\n",
       "      <th>Run Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Validation</th>\n",
       "      <td>0.971686</td>\n",
       "      <td>0.971686</td>\n",
       "      <td>0.971686</td>\n",
       "      <td>0.971686</td>\n",
       "      <td>5.750157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA Validation</th>\n",
       "      <td>0.924710</td>\n",
       "      <td>0.924710</td>\n",
       "      <td>0.924710</td>\n",
       "      <td>0.924710</td>\n",
       "      <td>4.788551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test</th>\n",
       "      <td>0.994335</td>\n",
       "      <td>0.994335</td>\n",
       "      <td>0.994335</td>\n",
       "      <td>0.969628</td>\n",
       "      <td>5.660717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA Test</th>\n",
       "      <td>0.984936</td>\n",
       "      <td>0.984936</td>\n",
       "      <td>0.984936</td>\n",
       "      <td>0.957857</td>\n",
       "      <td>5.188066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Accuracy  Avg Precision  Avg Recall    Avg F1  Run Time\n",
       "Validation      0.971686       0.971686    0.971686  0.971686  5.750157\n",
       "PCA Validation  0.924710       0.924710    0.924710  0.924710  4.788551\n",
       "Test            0.994335       0.994335    0.994335  0.969628  5.660717\n",
       "PCA Test        0.984936       0.984936    0.984936  0.957857  5.188066"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Overall\n",
    "\n",
    "val_res = [accuracy_val, avgprec_val, avgrecall_val, avgf1_val, elapsed_val]\n",
    "pca_val_res = [accuracy_val2, avgprec_val2, avgrecall_val2, avgf1_val2, elapsed_val2]\n",
    "test_res = [accuracy_test, avgprec_test, avgrecall_test, avgf1_test, elapsed_test]\n",
    "pca_test_res = [accuracy_test2, avgprec_test2, avgrecall_test2, avgf1_test2, elapsed_test2]\n",
    "\n",
    "pd.DataFrame([val_res] + [pca_val_res] + [test_res] + [pca_test_res], columns = [\"Accuracy\", \"Avg Precision\", \"Avg Recall\", \"Avg F1\", \"Run Time\"], index = [\"Validation\", \"PCA Validation\", \"Test\", \"PCA Test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, it seems that the overall accuracy / recall / precision / F1 all seem to be better when PCA is not applied to the data, but we will need to compare each individual score to get a better sensing of how the under-represented data are faring. The runtime, however, seem to be better when using the PCA data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STANDING</th>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.900813</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.872865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.985386</td>\n",
       "      <td>0.949333</td>\n",
       "      <td>0.944578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall Score</th>\n",
       "      <td>0.945392</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.956098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision Score</th>\n",
       "      <td>0.860248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.979253</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 STANDING  STAND_TO_SIT   SITTING  SIT_TO_STAND  STAND_TO_LIE  \\\n",
       "F1 Score         0.900813      0.500000  0.872865           0.0      0.645161   \n",
       "Recall Score     0.945392      0.333333  0.833333           0.0      0.588235   \n",
       "Precision Score  0.860248      1.000000  0.916335           0.0      0.714286   \n",
       "\n",
       "                   LAYING  LIE_TO_SIT  SIT_TO_LIE  LIE_TO_STAND   WALKING  \\\n",
       "F1 Score         0.989796    0.500000    0.583333      0.400000  0.985386   \n",
       "Recall Score     1.000000    0.545455    0.466667      0.285714  0.991597   \n",
       "Precision Score  0.979798    0.461538    0.777778      0.666667  0.979253   \n",
       "\n",
       "                 WALKING_DOWNSTAIRS  WALKING_UPSTAIRS  \n",
       "F1 Score                   0.949333          0.944578  \n",
       "Recall Score               0.956989          0.956098  \n",
       "Precision Score            0.941799          0.933333  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_test2] + [recall_test2] + [precision_test2], columns = activity_label, index = [\"F1 Score\", \"Recall Score\", \"Precision Score\"])\n",
    "pd.DataFrame([f1_val2] + [recall_val2] + [precision_val2], columns = activity_label, index = [\"F1 Score\", \"Recall Score\", \"Precision Score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STANDING</th>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.967851</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.963504</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.756757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.709677</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994652</td>\n",
       "      <td>0.987835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA F1 Score</th>\n",
       "      <td>0.900813</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.872865</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.989796</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.985386</td>\n",
       "      <td>0.949333</td>\n",
       "      <td>0.944578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall Score</th>\n",
       "      <td>0.976109</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA Recall Score</th>\n",
       "      <td>0.945392</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.956989</td>\n",
       "      <td>0.956098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision Score</th>\n",
       "      <td>0.959732</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.970588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989362</td>\n",
       "      <td>0.985437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA Precision Score</th>\n",
       "      <td>0.860248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916335</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.979798</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.979253</td>\n",
       "      <td>0.941799</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     STANDING  STAND_TO_SIT   SITTING  SIT_TO_STAND  \\\n",
       "F1 Score             0.967851      0.444444  0.963504      0.666667   \n",
       "PCA F1 Score         0.900813      0.500000  0.872865      0.000000   \n",
       "Recall Score         0.976109      0.333333  0.956522      0.500000   \n",
       "PCA Recall Score     0.945392      0.333333  0.833333      0.000000   \n",
       "Precision Score      0.959732      0.666667  0.970588      1.000000   \n",
       "PCA Precision Score  0.860248      1.000000  0.916335      0.000000   \n",
       "\n",
       "                     STAND_TO_LIE    LAYING  LIE_TO_SIT  SIT_TO_LIE  \\\n",
       "F1 Score                 0.756757  1.000000    0.740741    0.709677   \n",
       "PCA F1 Score             0.645161  0.989796    0.500000    0.583333   \n",
       "Recall Score             0.823529  1.000000    0.909091    0.733333   \n",
       "PCA Recall Score         0.588235  1.000000    0.545455    0.466667   \n",
       "Precision Score          0.700000  1.000000    0.625000    0.687500   \n",
       "PCA Precision Score      0.714286  0.979798    0.461538    0.777778   \n",
       "\n",
       "                     LIE_TO_STAND   WALKING  WALKING_DOWNSTAIRS  \\\n",
       "F1 Score                 0.421053  1.000000            0.994652   \n",
       "PCA F1 Score             0.400000  0.985386            0.949333   \n",
       "Recall Score             0.285714  1.000000            1.000000   \n",
       "PCA Recall Score         0.285714  0.991597            0.956989   \n",
       "Precision Score          0.800000  1.000000            0.989362   \n",
       "PCA Precision Score      0.666667  0.979253            0.941799   \n",
       "\n",
       "                     WALKING_UPSTAIRS  \n",
       "F1 Score                     0.987835  \n",
       "PCA F1 Score                 0.944578  \n",
       "Recall Score                 0.990244  \n",
       "PCA Recall Score             0.956098  \n",
       "Precision Score              0.985437  \n",
       "PCA Precision Score          0.933333  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_val] +[f1_val2] + [recall_val] + [recall_val2] + [precision_val] + [precision_val2], columns = activity_label, index = [\"F1 Score\", \"PCA F1 Score\", \"Recall Score\", \"PCA Recall Score\", \"Precision Score\", \"PCA Precision Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the validation set, it appears that generally for most of the scores, the PCA data seem to have a small drop, but some of the less represented data (e.g Stand to Sit) have rise in their precision scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STANDING</th>\n",
       "      <th>STAND_TO_SIT</th>\n",
       "      <th>SITTING</th>\n",
       "      <th>SIT_TO_STAND</th>\n",
       "      <th>STAND_TO_LIE</th>\n",
       "      <th>LAYING</th>\n",
       "      <th>LIE_TO_SIT</th>\n",
       "      <th>SIT_TO_LIE</th>\n",
       "      <th>LIE_TO_STAND</th>\n",
       "      <th>WALKING</th>\n",
       "      <th>WALKING_DOWNSTAIRS</th>\n",
       "      <th>WALKING_UPSTAIRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.993336</td>\n",
       "      <td>0.945055</td>\n",
       "      <td>0.992254</td>\n",
       "      <td>0.977778</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944000</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.895238</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998988</td>\n",
       "      <td>0.997671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA F1 Score</th>\n",
       "      <td>0.978783</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.973838</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.937853</td>\n",
       "      <td>0.997881</td>\n",
       "      <td>0.901639</td>\n",
       "      <td>0.930556</td>\n",
       "      <td>0.886792</td>\n",
       "      <td>0.997149</td>\n",
       "      <td>0.990389</td>\n",
       "      <td>0.989307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall Score</th>\n",
       "      <td>0.995081</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.990719</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA Recall Score</th>\n",
       "      <td>0.988756</td>\n",
       "      <td>0.914894</td>\n",
       "      <td>0.964424</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.922222</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.824561</td>\n",
       "      <td>0.998369</td>\n",
       "      <td>0.991895</td>\n",
       "      <td>0.991612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision Score</th>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.977273</td>\n",
       "      <td>0.993794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935484</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907692</td>\n",
       "      <td>0.934211</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997978</td>\n",
       "      <td>0.997207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PCA Precision Score</th>\n",
       "      <td>0.969008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983438</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954023</td>\n",
       "      <td>0.995772</td>\n",
       "      <td>0.887097</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.959184</td>\n",
       "      <td>0.995932</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     STANDING  STAND_TO_SIT   SITTING  SIT_TO_STAND  \\\n",
       "F1 Score             0.993336      0.945055  0.992254      0.977778   \n",
       "PCA F1 Score         0.978783      0.955556  0.973838      0.954545   \n",
       "Recall Score         0.995081      0.914894  0.990719      0.956522   \n",
       "PCA Recall Score     0.988756      0.914894  0.964424      0.913043   \n",
       "Precision Score      0.991597      0.977273  0.993794      1.000000   \n",
       "PCA Precision Score  0.969008      1.000000  0.983438      1.000000   \n",
       "\n",
       "                     STAND_TO_LIE    LAYING  LIE_TO_SIT  SIT_TO_LIE  \\\n",
       "F1 Score                 0.950820  1.000000    0.944000    0.940397   \n",
       "PCA F1 Score             0.937853  0.997881    0.901639    0.930556   \n",
       "Recall Score             0.966667  1.000000    0.983333    0.946667   \n",
       "PCA Recall Score         0.922222  1.000000    0.916667    0.893333   \n",
       "Precision Score          0.935484  1.000000    0.907692    0.934211   \n",
       "PCA Precision Score      0.954023  0.995772    0.887097    0.971014   \n",
       "\n",
       "                     LIE_TO_STAND   WALKING  WALKING_DOWNSTAIRS  \\\n",
       "F1 Score                 0.895238  1.000000            0.998988   \n",
       "PCA F1 Score             0.886792  0.997149            0.990389   \n",
       "Recall Score             0.824561  1.000000            1.000000   \n",
       "PCA Recall Score         0.824561  0.998369            0.991895   \n",
       "Precision Score          0.979167  1.000000            0.997978   \n",
       "PCA Precision Score      0.959184  0.995932            0.988889   \n",
       "\n",
       "                     WALKING_UPSTAIRS  \n",
       "F1 Score                     0.997671  \n",
       "PCA F1 Score                 0.989307  \n",
       "Recall Score                 0.998136  \n",
       "PCA Recall Score             0.991612  \n",
       "Precision Score              0.997207  \n",
       "PCA Precision Score          0.987013  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([f1_test] +[f1_test2] + [recall_test] + [recall_test2] + [precision_test] + [precision_test2], columns = activity_label, index = [\"F1 Score\", \"PCA F1 Score\", \"Recall Score\", \"PCA Recall Score\", \"Precision Score\", \"PCA Precision Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the test set similarly to the validation set, it appears that generally for most of the scores, the PCA data seem to have a drop, but some of the less represented data (e.g Stand to Sit) have rise in their precision scores.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52d0e4b0fefd94b2cb6980e387e638c0c22f7eeec79f53bd58ff8a79f97fe231"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
